import os
import sys
import random
import numpy as np
import mysql.connector
from collections import defaultdict
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import MinMaxScaler  # <--- å¿…é¡»ç”¨è¿™ä¸ªï¼

# ================= 1. è·¯å¾„ä¿®å¤ä¸ç¯å¢ƒé…ç½® =================
current_dir = os.path.dirname(os.path.abspath(__file__))
root_dir = os.path.dirname(current_dir)
if root_dir not in sys.path:
    sys.path.insert(0, root_dir)

from recommend.config import DB_CONFIG, logger
from recommend.services.matrix_factorization import build_svd_model, get_svd_recommendations
from recommend.services.neural_cf import build_ncf_model, get_ncf_recommendations, TF_AVAILABLE
from recommend.services.content_based import build_service_similarity_matrix, get_content_based_recommendations


# ================= 2. æ ¸å¿ƒæ•°æ®å¤„ç†å‡½æ•° (ä¿æŒä¸å˜) =================
# ... (load_and_split_data, build_interaction_matrix ä»£ç å¤ç”¨ä½ ä¹‹å‰çš„å³å¯ï¼Œæ— éœ€ä¿®æ”¹) ...
def load_and_split_data():
    print("ğŸ”„ æ­£åœ¨åŠ è½½æ•°æ®åº“æ•°æ®...")
    conn = mysql.connector.connect(**DB_CONFIG)
    cursor = conn.cursor(dictionary=True)
    cursor.execute("SELECT user_id, service_id, behavior_type, created_at FROM user_behavior ORDER BY created_at ASC")
    behaviors = cursor.fetchall()
    cursor.execute("SELECT id, category_id, tags, description, price, rating FROM service")
    services = {row['id']: row for row in cursor.fetchall()}
    conn.close()

    total = len(behaviors)
    split_1 = int(total * 0.6)
    split_2 = int(total * 0.8)
    return behaviors[:split_1], behaviors[split_1:split_2], behaviors[split_2:], services


def build_interaction_matrix(behavior_list):
    matrix = defaultdict(dict)
    for item in behavior_list:
        uid, sid = item['user_id'], item['service_id']
        b_type = item['behavior_type']
        weight = 1.0
        if b_type == 'click':
            weight = 2.0
        elif b_type == 'favorite':
            weight = 3.0
        elif b_type == 'order':
            weight = 5.0
        # é™åˆ¶ä¸Šé™ï¼Œé…åˆ SVD log1p
        matrix[uid][sid] = min(matrix[uid].get(sid, 0.0) + weight, 10.0)
    return matrix


# ================= 3. ç‰¹å¾å·¥ç¨‹ (ä¿®å¤ç‰ˆ) =================

def generate_meta_features(train_base, train_meta, services):
    print("\nğŸš€ ç¬¬ä¸€é˜¶æ®µï¼šè®­ç»ƒåŸºæ¨¡å‹ (Base Models)...")

    base_matrix = build_interaction_matrix(train_base)
    users = list(base_matrix.keys())
    service_ids = list(services.keys())

    print("   -> æ­£åœ¨è®­ç»ƒ SVD...")
    svd_model = build_svd_model(base_matrix, users, service_ids)

    print("   -> æ­£åœ¨è®­ç»ƒ NCF...")
    ncf_model = None
    mock_feats = {u: {} for u in users}
    if TF_AVAILABLE:
        # Epochs=10 å·²ç»è¶³å¤Ÿï¼Œä¹‹å‰éªŒè¯è¿‡ Acc 98%
        ncf_model = build_ncf_model(base_matrix, mock_feats, services, epochs=10)

    print("   -> æ­£åœ¨æ„å»º CB çŸ©é˜µ...")
    sim_matrix = build_service_similarity_matrix(services)

    print("\nğŸš€ ç¬¬äºŒé˜¶æ®µï¼šç”Ÿæˆå…ƒæ•°æ®ç‰¹å¾ (Meta Features)...")
    X = []
    y = []

    meta_interactions = build_interaction_matrix(train_meta)
    meta_users = list(meta_interactions.keys())

    # å…³é”®ï¼šè®¾ç½®ä¸€ä¸ªè¶³å¤Ÿå¤§çš„ Limitï¼Œç¡®ä¿æ‹¿åˆ°æ‰€æœ‰åˆ†æ•°
    # ä½ çš„æœåŠ¡æ€»æ•°åªæœ‰ 553ï¼Œè®¾ä¸º 1000 è¶³å¤Ÿå®‰å…¨
    limit_n = 2000

    count = 0
    for uid in meta_users:
        if uid not in base_matrix: continue

        true_items = set(meta_interactions[uid].keys())

        # --- æ‰¹é‡è·å–åŸºæ¨¡å‹é¢„æµ‹åˆ† ---
        # 1. CB
        cb_recs = get_content_based_recommendations(uid, base_matrix, sim_matrix, limit=limit_n)
        cb_map = {sid: score for sid, score in cb_recs}

        # 2. SVD
        svd_recs = []
        if svd_model:
            svd_recs = get_svd_recommendations(svd_model, uid, base_matrix, limit=limit_n)
        svd_map = {sid: score for sid, score in svd_recs}

        # 3. NCF
        ncf_recs = []
        if ncf_model:
            ncf_recs = get_ncf_recommendations(ncf_model, uid, base_matrix, services, limit=limit_n)
        ncf_map = {sid: score for sid, score in ncf_recs}

        # --- æ­£æ ·æœ¬ ---
        for sid in true_items:
            vec = [
                cb_map.get(sid, 0.0),
                svd_map.get(sid, 0.0),
                ncf_map.get(sid, 0.0)
            ]
            X.append(vec)
            y.append(1)

        # --- è´Ÿæ ·æœ¬ ---
        neg_candidates = [s for s in service_ids if s not in true_items]
        if neg_candidates:
            # 1:1 é‡‡æ ·
            n_neg = len(true_items)
            chosen_negs = random.sample(neg_candidates, k=min(n_neg, len(neg_candidates)))
            for sid in chosen_negs:
                vec = [
                    cb_map.get(sid, 0.0),
                    svd_map.get(sid, 0.0),
                    ncf_map.get(sid, 0.0)
                ]
                X.append(vec)
                y.append(0)

        count += 1
        if count % 200 == 0:
            print(f"   ...å·²å¤„ç† {count} ä¸ª Meta ç”¨æˆ·")

    return np.array(X), np.array(y)


# ================= 4. æƒé‡è®­ç»ƒ (ä¿®å¤ç‰ˆ) =================

def train_meta_learner(X, y):
    print("\nğŸš€ ç¬¬ä¸‰é˜¶æ®µï¼šè®­ç»ƒæƒé‡æ¨¡å‹ (Logistic Regression)...")

    if len(X) == 0: return [0.33, 0.33, 0.33]

    # --- å…³é”®ä¿®å¤ï¼šä½¿ç”¨ MinMaxScaler ---
    # è¿™ä¼šæŠŠç‰¹å¾ç¼©æ”¾åˆ° [0, 1] åŒºé—´ï¼Œé…åˆ fit_intercept=False æ•ˆæœæœ€ä½³
    scaler = MinMaxScaler()
    X_scaled = scaler.fit_transform(X)

    # C=10.0: å‡å¼±æ­£åˆ™åŒ–ï¼Œè®©æ¨¡å‹æ›´ç›¸ä¿¡æ•°æ®
    # positive=True: å¼ºåˆ¶è¦æ±‚ç³»æ•°ä¸ºæ­£ (Sklearn 0.24+ æ”¯æŒ)ï¼Œè¿™æ˜¯ç‰©ç†æ„ä¹‰ä¸Šçš„çº¦æŸï¼
    # å¦‚æœä½ çš„ sklearn ç‰ˆæœ¬æ—§ä¸æ”¯æŒ positive=Trueï¼Œä¹Ÿæ²¡å…³ç³»ï¼Œä¸‹é¢çš„ maximum(0) ä¼šå¤„ç†
    try:
        clf = LogisticRegression(fit_intercept=False, solver='lbfgs', C=10.0, positive=True)
        clf.fit(X_scaled, y)
    except TypeError:
        # å…¼å®¹æ—§ç‰ˆæœ¬ sklearn
        clf = LogisticRegression(fit_intercept=False, solver='lbfgs', C=10.0)
        clf.fit(X_scaled, y)

    raw_weights = clf.coef_[0]
    print(f"   [åŸå§‹ç³»æ•°] CB: {raw_weights[0]:.4f}, SVD: {raw_weights[1]:.4f}, NCF: {raw_weights[2]:.4f}")

    # --- åå¤„ç† ---
    weights = np.maximum(raw_weights, 0)
    total_w = np.sum(weights)

    if total_w > 0:
        weights = weights / total_w
    else:
        weights = np.array([0.33, 0.33, 0.34])

    print("\n" + "=" * 50)
    print("ğŸ¯ æœ€ç»ˆæ¨èç®—æ³•æœ€ä½³æƒé‡ç»„åˆ")
    print("=" * 50)
    print(f"  ğŸ“Œ Content-Based (CB) : {weights[0]:.4f}")
    print(f"  ğŸ“Œ SVD (Matrix Factor): {weights[1]:.4f}")
    print(f"  ğŸ“Œ NCF (Deep Learning): {weights[2]:.4f}")
    print("=" * 50)

    return weights


if __name__ == "__main__":
    train_base, train_meta, test_final, services = load_and_split_data()
    X, y = generate_meta_features(train_base, train_meta, services)
    best_weights = train_meta_learner(X, y)